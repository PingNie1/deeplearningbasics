## 前言
上一篇写的[神经网络调试Checklist](https://mp.weixin.qq.com/s/Py0NIqPor7i92b7pS5Sw_A)详细地介绍了神经网络调试的步骤和技巧。本文将进一步学习深度学习网络中另一核心内容：**求导和反向传播**。笔者认为，在熟练使用诸如Pytroch/Tensorflow等深度学习工具之外，咱们也有必要对背后的数学知识有所理解。因此，本文将从简单的标量谈起，深入解析神经网络中张量求导和反向传播所涉及的数学计算，希望能成为深度学习初学者的参考资料。

提几个问题，读者朋友看下是否可以自如回答：
- 1. 什么是链式法则？
- 2. 什么是Jacobin矩阵，它有什么用处？
- 3. 梯度的定义是什么？方向导数和梯度的关系是什么？
- 4. 神经网络中张量反向传播有什么特点？哪些特性保证了神经网络中高效的梯度计算？

本文的目录如下：

- 0.0 前言
- 1.1 标量、向量、矩阵、张量
- 1.2 简单的线性代数
- 2.1 标量：导数的概念
- 2.2 标量：求导法则
- 2.3 标量：求导常用公式
- 2.4 多个标量：多元函数求导、偏导数
- 2.5 方向导数和梯度
- 2.7 向量的梯度和Jacobian矩阵
- 2.8 矩阵、张量的梯度和Jacobian矩阵
- 2.9 神经网络中几个实用的梯度计算
- 3.1 神经网络中的反向传播
- 3.2 单层神经网络的梯度计算例子
- 4.1 Pytorch自动求梯度
- 4.2 Tensorflow自动求梯度

## 1.1 标量、向量、矩阵、张量
**标量** 简单来说：一个标量就是一个单独的数。
日常生活的一个例子：我们去超市买了1个冰淇凌，花费了2元，排队结账用了5分钟，其中1、2和5都是标量。

对于标量，我们一般不讨论其维度或者形状(shape)的大小（或者说是0维）。

**向量**
将多个标量放在一起组成了向量。比如$x=[1,2]$就是一个向量。一个二维平面的点位置有2个特征：横坐标和纵坐标，因此可以用$[x坐标，y坐标]$这样一个二维向量来表示二维平面上的一个点；同理，可以使用[x坐标，y坐标，z坐标]三维向量来表示空间中的一个点所在的位置。日常生活中，我们也可以使用$[身高h,性别s,体重w,肤色f]$这样一个4维向量来表示一个人的特点。更高维的向量可以表达一个物体更多维度的特征。

向量的维度通常表达的是向量中标量的个数。$[1,2，3]$的维度为3维。

**矩阵** 将多个向量排列到一起可以组成矩阵。例如：
$$\begin{bmatrix}
[1 & 2], \\
[1 & 3] 
\end{bmatrix} \tag{1-1-1}$$

如矩阵(1-1-1)所示，矩阵的每一行就是一个向量。

矩阵的**形状**：每一列标量的个数 x 每一行标量的个数，矩阵(1-1-1)形状为2x2，维度为2。

**张量** 
将多个矩阵组合到一起可以形成张量。比如：
$$\begin{bmatrix} 
\begin{bmatrix}
[1 & 2] \\
[1 & 2] 
\end{bmatrix}, \\
\begin{bmatrix}
[2 & 3] \\
[2 & 3] 
\end{bmatrix}, \\
\begin{bmatrix}
[3 & 3] \\
[3 & 4] 
\end{bmatrix} \\
\end{bmatrix} \tag{1-1-2}$$
因此标量、向量、矩阵都可以看作是维度更少的张量。

张量的形状：假设张量A形状为$d_n\times d_{n-1}... \times d_1$，那么表达的含义是：$n$维张量A中包含了$d_{n}$个形状为$d_{n-1} ... \times d_1$的$n-1$维张量B，并以此类推到1维张量。所以张量(1-1-2)的形状是3x2x2维度是3，张量(1-1-1)的形状是是2x2维度是2。

注意：由于大家的对矩阵、张量的形状和维度的概念容易混淆，本文在描述张量形状的时候都会使用$\times$符号来区分不同的维度。

**Pytorch实践**
```
import torch
scalar = torch.tensor(5)
vector = torch.tensor([5,5,5])
matrix = torch.tensor([[5,2],[5,3]])
tensor = torch.tensor([[[5,2],[5,3]], [[1,2],[2,3]]])
print("scalar: {}, shape: {}".format(scalar, scalar.shape))
print("vector: {}, shape: {}".format(vector, vector.shape))
print("matrix: {}, shape: {}".format(matrix, matrix.shape))
print("tensor: {}, shape: {}".format(tensor, tensor.shape))
"""
scalar: 5, shape: torch.Size([])
vector: tensor([5, 5, 5]), shape: torch.Size([3])
matrix: tensor([[5, 2],
        [5, 3]]), shape: torch.Size([2, 2])
tensor: tensor([[[5, 2],
         [5, 3]],

        [[1, 2],
         [2, 3]]]), shape: torch.Size([2, 2, 2])
"""
```

## 1.2 简单的线性代数

1.1章节中我们谈到向量可以延伸到矩阵，然后延伸到张量，因此为了能够更好得对深度学习中的向量/矩阵/张量运算、梯度求解有一个更清晰的理解，我们需要学习一下基本的线性代数知识。由于张量运算过于复杂，一般我们将张量运算交给计算机进行。我们在进行线性代数讲解的时候也尽可能使用特殊而又简单的张量（也就是矩阵和向量）。

**什么是线性代数**

一般来说，线性代数在是本科一年级的一门必修课。但是本文将不会涉及完整的线性代数体系知识，仅仅将线性代数常用于深度学习的部分知识进行整理。我们将线性代数看作深度学习中的数学计算工具即可：深度学习使用线性代数对张量进行加减乘除等各种运算。

**线性代数能带来什么好处？**

由于深度学习中需要进行大量的张量运算，而每个张量又包含了多个标量，因此可以想象，如果直接进行标量计算需要大量的$for$循环。但是线性代数的知识，能将复杂而又慢的循环遍历标量计算变得更加优雅、快速、简单。

**例子**
```
# 将两个向量中的每个元素相乘求和
x = [1,2,3]
y = [2,3,4]
sum = 0.0
for i in range(len(x)):
    sum = sum + x[i]*y[i]
# 借助深度学习工具torch，使用点乘直接运算
import torch
x = torch.tensor([1,2,3])
y = torch.tensor([2,3,4])
sum = torch.dot(x, y))
```

**深度学习中的应用**

深度学习中的神经网络包含了大量的参数（也叫做parameters/weights）和参数计算。参数往往以张量的形式储存，线性代数的知识（比如点乘，矩阵运算）能让参数间的计算更加快速（如果是使用GPU进行计算，这种提速会更加明显）。

**标量运算**

最简单的形式：标量和标量进行加减乘除运算。

*例子：标量+标量*

```
import torch
a = torch.tensor(2)
b = torch.tensor(3)
c = a + b
print(c)
# tensor(5)
```

**向量和标量**

将向量中的每一个标量都与另一个标量都进行计算（[broadcasting/广播](https://pytorch.org/docs/stable/notes/broadcasting.html)的一种）。

*例子*

```
a = torch.tensor(2)
b = torch.tensor([1,2,3])
c = a + b
print(c)
# tensor([3, 4, 5])
```

**向量和向量按位置（elementwise）运算**

向量间同一个位置的元素/标量进行运算。

*例子*
```
a = torch.tensor([2,3,4])
b = torch.tensor([1,2,3])
c = a + b
print(c)
# tensor([3, 5, 7])
# Hadamard product
a = torch.tensor([2,3,4])
b = torch.tensor([1,2,3])
c = a * b
print(c)
# tensor([ 2,  6, 12])
```

**点乘**

向量之间的点乘是深度学习中最普遍也是最常用的一种计算:先按位置做乘法，然后相加求和。

*例子*
```
a = torch.tensor([2,3,4])
b = torch.tensor([1,2,3])
c = torch.dot(a, b)
print(c)
# tensor(20)
```

**矩阵和标量**

将矩阵中的每一个标量都与另一个标量都进行计算（[broadcasting/广播](https://pytorch.org/docs/stable/notes/broadcasting.html)的一种）。

*例子*

```
a = torch.tensor(2)
b = torch.tensor([[1,2,3],
                  [1,2,3]])
c = a + b
print(c)
# tensor([[3, 4, 5],
#        [3, 4, 5]])
```

**矩阵和向量**

将向量广播成与矩阵相同的维度，然后都矩阵的每一个元素都进行计算（[broadcasting/广播](https://pytorch.org/docs/stable/notes/broadcasting.html)的一种）。

*例子*

```
a = torch.tensor(2)
b = torch.tensor([[1,2,3],
                  [1,2,3]])
c = a + b
print(c)
# tensor([[3, 4, 5],
#        [3, 4, 5]])
```

**矩阵和矩阵按位置（elementwise）运算**

向量间同一个位置的元素/标量进行运算。

*例子*
```
a = torch.tensor([[2,3,4],
                  [2,3,4]])
b = torch.tensor([[1,2,3]])
# 现将b变成[[1,2,3],
#          [1,2,3]]再计算
c = a + b
print(c)
# tensor([[3, 5, 7],
#        [3, 5, 7]])
# Hadamard product
```

**矩阵乘法**

深度学习中最常用、最重要的计算！
假设矩阵$a \in \mathbb R ^{n \times m}, b \in \mathbb R^{m \times k}$，那么$c = a \times b \in \mathbb R^{n \times k}$。对于$c$矩阵中的每一个元素计算方式为：$a$的第$i$行向量与$b$的第$j$列向量进行点乘。

$$c_{ij} = \sum_k {a_{ik}b_{kj}}$$

因此两个矩阵要进行矩阵乘法必须满足：前一个矩阵的列数=后一个矩阵的行数。

*例子*
```
a = torch.tensor([[2,3,4],
                  [2,3,4]])
print(a.shape)
# torch.Size([2, 3])
b = torch.tensor([[1],
                  [2],
                  [3]])
print(b.shape)
# torch.Size([3, 1])
c = torch.matmul(a, b)
print(c)
#tensor([[20],
#        [20]])
print(c.shape)
# torch.Size([2, 1]) 
```
**转置**

前面提到了矩阵乘法，因此为了能够让矩阵乘法正常计算，我们常常需要对矩阵进行转置来得到正确的矩阵形状。比如一个$\mathbb R^{2 \times 3}$的矩阵无法和$\mathbb R^{2 \times 3}$直接做乘法，但可以将后一个转置得到$\mathbb R^{3 \times 2}$后便可以进行矩阵乘法了。具体的如何进行转置呢？

假设矩阵为
$$M = \begin{bmatrix}
 [a& b &c], \\ 
 [d& e & f]
\end{bmatrix}$$

那么它的转置为
$$M = \begin{bmatrix}
 [a& d], \\ 
 [b& e], \\
 [c&f ]
\end{bmatrix}$$

- 先将矩阵$M$逆时针旋转$90^0$
- 然后每一列顺序反过来即可，比如第一列：$[b,a,c] \to [a,b,c]$

```
import torch
a = torch.tensor([[2,3,4],
                  [2,3,4]])
print(a.shape)
b = torch.tensor([[2,3,4],
                  [2,3,4]])
print(b.shape)
c = b.T
print(c.shape)
d = torch.matmul(a, c)
print(d)
# torch.Size([2, 3])
# torch.Size([2, 3])
# torch.Size([3, 2])
# tensor([[29, 29],
#        [29, 29]])
```

## 2.1 标量：导数的概念
**导数的定义**：设函数$y=f(x), \mathbb R \to \mathbb R$，将实数$x$映射到实数$y$, 在点$x_0$的某领域内有定义，当自变量在$x_0$处取得增量$\Delta x$时函数取得相应的增量$\Delta y=f(x_0 + \Delta x) - f(x_0)$，如果极限

$$\lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} \tag{2.1.1}$$

存在，则称$f(x)$在点$x_0$处可导，通常可以写做$f'(x_0)$, $\frac{dy}{dx} \vert_{x=x_0}$, $\frac{\partial f}{\partial x}$或者$\frac{df}{dx} \vert_{x=x_0}$。

**注意本文在一元函数导数表示时候也用$\frac{\partial f}{\partial x}$**
![图2.1.1 导数示意图，来源：https://www.mathsisfun.com/calculus/derivatives-introduction.html](https://www.mathsisfun.com/calculus/images/slope-dy-dx2.svg)

于是，如图2.1.1所示，通俗得讲：对于定义域内的$x$，当$\Delta x$很小的时候，在点$x$处的导数$f'(x)$就是点x处的斜率$\frac{\Delta y}{\Delta x}$（导数的几何意义）。

**举例**: 计算$f(x)=x^2$的导数，根据定义

$$\lim_{\Delta x \to 0} = \frac{f(x+\Delta x) -f(x)}{\Delta x} = \frac{2x\Delta x + {\Delta x}^2}{\Delta x} = 2x + {\Delta x}^2 \tag{2.1.2}$$

由于$\Delta x$无限趋近于0，那么可以得到$f'(x)=2x$。

**读者可继续选修左导数、右导数的定义**。


**函数$f(x)$在点$x_0$处可求导的充分必要条件是$f(x)$在点$x_0$处的左、右导数都存在且相等。**

观察2.1.1可以继续得到:

$$f(x+\Delta x) \approx f(x) + \frac{\Delta y}{\Delta x} \Delta x = f(x) + f'(x)\Delta x = f(x) + \frac{\partial f}{\partial x} \Delta x \tag{2.1.3}$$

即：$x$的增量$\Delta x$与$y$的变化量$\Delta y$可以通过导数联系起来，当$x$增加$\Delta x$的时候，$y=f(x)$近似地增加$\frac{\partial y}{\partial x} \Delta x$。

$$x \to x + \Delta x \Rightarrow y \to \approx y + \frac{\partial y}{\partial x} \Delta x \tag{2.1.4}$$

## 2.2 标量：求导链式法则
一般来说，通过2.1章节导数的定义可以求出导数，但为了计算方便和快捷，我们通常需要知道基本的求导法则和常用公式。

**函数的和、差、积、商求导法则**

*定理1* 如果函数$u = u(x), v = v(x)$在点$x$处可导，则$u(x) \pm v(x)$, $Cu(x)(C是常数)$, $u(x)v(x)$, $\frac{u(x)}{v(x)}(当v(x) \neq 0)$都在点$x$处可导，并且：

$$(u(x) \pm v(x) )' = u'(x) + v'(x) \tag{2.2.1}$$
$$(Cu(x))' = Cu'(x)$$
$$(u(x)v(x))' = u'(x)v(x) + u(x)v'(x)$$
$$(\frac{u(x)}{v(x)})' = \frac{u'(x)v(x) - u(x)v'(x)}{v^2(x)}, 特别地\frac{1}{v(x)} = \frac{-v'(x)}{v^2}$$

**复合函数求导：链式法则**

根据导数的定义可以知道单个函数$f(x)$的求导方法，当$f(x)$更加复杂时（比如$f(x) = \frac{sinx}{x^2 + cosx}$），直接求导将变得困难。因此，我们希望能将一步复杂的函数拆分成多步简单的函数求导。



*链式法则* 告诉我们如何对一个复杂函数进行多步求导。假设我们有实数函数$z=g(y), y=f(x)$其中$f,g: \mathbb R \to \mathbb R$ 

$$x \overset{f}{\rightarrow} y \overset{g}{\rightarrow} z$$
*定理2 链式法则*：
$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x} \tag{2.2.2}$$

结合公式(2.1.4)，我们可以知道：
$$x \to x + \Delta x \Rightarrow y \to \approx y + \frac{\partial y}{\partial x}\Delta x \tag{2.2.3}$$

$$y \to y + \Delta y \Rightarrow z \to z + \frac{\partial z}{\partial y}\Delta y$$

从公式(2.2.2)可以看出$x$的增量变化对$z$的影响：如果$x$增加$\Delta x$，$y$增加$\Delta y = \frac{\partial y}{\partial x} \Delta x$；进一步，$y$增加$\Delta y$，$z$增加$\Delta z = \frac{\partial z}{\partial y} \Delta y = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$

**举例**
$z = e^{2x}$可以写成$z = e^y, y = 2x$，那么
$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x} = e^{2x} * 2= 2e^{2x}$$

## 2.3 标量：求导法常用公式


**常用求导公式**
$$(C)'=0$$
$$(x^a)' = ax^{a-1} a为实数$$
$$(a^x)' = a^x lna, (e^x)' = e^x$$
$$(log_ax)' = \frac{1}{xlna}, (ln|x|)' = \frac{1}{x}$$
$$(sinx)' = cosx$$
$$(cosx)' = -sinx$$
$$(tanx)' = \frac{1}{cos^2x}$$
$$(cotx)' = -\frac{1}{sin^2x}$$
$$(arcsinx)' = \frac{1}{\sqrt{1 - x^2}}$$
$$(arccosx)' = - \frac{1}{\sqrt{1 - x^2}}$$
$$(arctanx)' = \frac{1}{1 + x^2}$$
$$(\frac{e^x + e^{-x}}{2})' = \frac{e^x - e^{-x}}{2}$$
$$(\frac{e^x - e^{-x}}{2})' = \frac{e^x + e^{-x}}{2}$$
$$(ln(x + \sqrt{x^2 + 1}))' = \frac{1}{\sqrt{x^2 + 1}}$$
$$(ln(x + \sqrt{x^2 - 1}))' = \frac{1}{\sqrt{x^2 - 1}}$$

## 2.4 多个标量：多元函数求导、偏导数

**一元函数**：一个自变量输入为$x$，一个因变量输出为$y$。

**多元函数**：多个自变量输入为$x, y$，一个因变量输出为$y$。

为了我们能够在多个变量的函数中考虑只有一个自变量变化的变化率，我们需要定义一个叫做**偏导数**的概念。

*定义* 设函数$z=f(x, y)$在点$(x_0, y_0)$的某领域内有定义，将$y$固定为$y_0$，而$x$在$x_0$处取得增量$\Delta x$ 时，$f(x, y)$所产生的相应增量：

$$\Delta_x z = f(x_0 + \Delta x, y_0) - f(x_0, y_0) \tag{2.4.1}$$

称为$f(x, y)$关于$x$的偏增量，如果极限：
$$\lim_{\Delta x \to 0} \frac{\Delta_x z}{\Delta x} = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x, y_0) - f(x_0, y_0)}{\Delta x} \tag{2.4.2}$$

存在，则称此极限为函数$z=f(x, y)$在点$(x_0, y_0)$处对$x$的偏导数，记做：
$$\frac{\partial z}{ \partial x} \vert_{x_0, y_0}, \frac{\partial f}{ \partial x} \vert_{x_0, y_0}, f'_x(x_0, y_0) \tag{2.4.3}$$

也就是：
$$f'_x(x_0, y_0) = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{ \Delta x}$$

同理，可以得到函数$f(x, y)$对$y$的偏导数：
$$f'_y(x_0, y_0) = \lim_{\Delta y \to 0} \frac{f(y_0 + \Delta y) - f(y_0)}{ \Delta y}$$

如果函数$z=f(x, y)$在区域$D$内每一点$(x,y)$都有对$x$的偏导数$f'_x(x,y)$或对$y$的偏导数$f'_y(x,y)$，且$f'_x(x,y)$和$f'_y(x,y)$仍是$x,y$的函数，我们将其偏导函数（简称偏导数），可以记做：
$$\frac{\partial z}{\partial x},\frac{\partial f}{\partial x} \tag{2.4.5}$$
$$\frac{\partial z}{\partial y},\frac{\partial f}{\partial y}$$

**偏导数的定义可以推广到三元以上函数的情形。根据偏导数的定义，当多元函数表达式已经给出的时候，求他的各个偏导数并不需要新方法，将其他变量固定为常量，对单个变量利用一元函数求导方法求导即可**

**举例**

*求$z=5x_0^2 + x_0x_1$在点$(x_0=1,x_1=2)$处的偏导数*.

先将$x_1$看作常量，对$x_0$求导得到：

$$\frac{\partial z}{\partial x_0} = 10x_0 + x_1$$

将$x_0$堪称常量，对$x_1$求导得到：

$$\frac{\partial z}{\partial x_1} = x_0$$

于是带入$x_0=1, x_1 = 2$到上式中得到：
$$\frac{\partial z}{\partial x_0} \vert _{1,2} = 12, \frac{\partial z}{\partial x_1} \vert _{1,2} = 2$$

**推广到向量**

在章节1.1中我们谈到：将多个标量放在一起可以形成向量。那么，我们重新从向量角度重新看一眼偏导数。我们将$N$个自变量放到一起可以组成一个向量$X=[x_0, x_1,..., x_N]$，输出得到一个标量$y$。于是我们的函数$f(X), \mathbb R^N \to \mathbb R$ 输入是一个向量（多个自变量），输出是一个标量。于是，在任意一点$X \in \mathbb R$, $y$对$X$的偏导数$\frac{\partial y}{\partial X} = [\frac{\partial y}{\partial x_0}, \frac{\partial y}{\partial x_1},..., \frac{\partial y}{\partial x_N}]$.

**复合函数的偏导数**

对于一元函数，我们可以使用链式法则对复合函数进行求导。对于多元函数，由于复合函数的构成可以更加复杂，因此无法针对所有情况给出通用的求导公式，但是可以针对其中一种情况给出求导公式，方便大家发现读者发现其中的求导规律。

**定理1 如果函数$u=\varphi(x, y), v=\psi(x, y)$ 在点$(x,y)$的各偏导数都存在，$z=f(u, v)在对应点$(u,v)可微，则复合函数$z=f(\varphi(x,y), \psi(x,y))$在点$(x, y)$的偏导数存在，且**
$$\frac{\partial z}{\partial x}= \frac{\partial z}{\partial u} \frac{\partial u}{\partial x} + \frac{\partial z}{\partial v}\frac{\partial v}{\partial x}$$
$$\frac{\partial z}{\partial y}= \frac{\partial z}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial z}{\partial y}\frac{\partial v}{\partial y} \tag{2.4.6}$$

**举例**

假设函数$F(x,y,z) = xz + yz，令u(x,z) = xz, v(y,z)=yz，F(u,v) = u + v$那么$F,u,v,x,y,z)之间的计算关系可以如图2.4.1所示,结构图2.4.1显示$z$不仅和$u$相关，也和$v$相关，于是我们试着计算一下偏导数

$$\frac{\partial F}{\partial z} = \frac{\partial F}{\partial u} \frac{\partial u}{\partial z} + \frac{\partial F}{\partial v} \frac{\partial v}{\partial z} = x + y$$

![图2.4.1：多元复合函数求偏导：知乎张敬信](https://pic3.zhimg.com/v2-ab40835bee904e1b508903cc024afb96_b.jpg)


## 2.5 方向导数和梯度

学习了偏导数的概念之后，我们可以进一步学习方向导数的概念。

在实际问题中，我们通常要研究函数在某一点处沿着某一方向的变化率（之前学习的对$x,y,z$的偏导数其实是沿着$x,y,z$方向的导数），例如在大气气象的研究中，我们就需要研究温度、气压沿着不同方向的变化率，方向导数便可以用来描述这类变化率。

![图2.5.1 方向导数示意图](https://pic3.zhimg.com/v2-66d67c93ad6cc6ae6bbc2f707fb3ef52_b.jpg)

设函数$z=f(x,y)$在点$P(x,y)$的某领域内有定义，如图2.5.1所示，从点$P$出发在$xOy$平面上引出一条射线$l$，并且$e$是射线$l$上的单位向量。假设$e$与$x$轴和$y$轴的夹脚分别为$\alpha, \beta$，从而单位向量$e=(cos\alpha, cos \beta)$，在射线上另外再取一个点$P'(x+\Delta x, y+ \Delta y)，记：
$$\rho = |PP'|=\sqrt{\Delta x^2 + \Delta y^2} \tag{2.5.1}$$
$$\Delta_l z = f(P') - f(P) = f(x+\Delta x, y+ \Delta y) - f(x,y)$$

那么当$\rho$较小的时候，$\frac{\Delta_l z}{\rho}$可以近似地反应函数$z=f(x,y)$在点$P$处沿着射线$l$方向的变化情况，而其极限可以精确地描述这个变化情况，因此有如下定义：

**定义 在上面所给出的假设下，如果极限**

$$\lim_{\rho \to 0} \frac{\Delta_l z}{\rho} = \lim_{\rho \to 0} \frac{f(x+\Delta x, y+ \Delta y) - f(x,y)}{\rho} \tag{2.5.2}$$

**存在，则称此极限为函数$z=f(x,y)$在点点$P$处沿着方向$l或者e$的方向导数，记做$\frac{\partial z}{\partial l}或者\frac{\partial f}{\partial l}$**.

方向导数$\frac{\partial z}{\partial l}$反映了沿着射线$l$方向变量$z$的变化情况。

**偏导数与方向导数的关系：**如果$i$表示$x$轴的方向，那么根据方向函数的定义，当$e=i$的时候，由于$\Delta_l z = \Delta_x z, \rho=\Delta x$，从而：

$$\frac{\partial z}{\partial l} = \lim_{\rho \to 0} \frac{\Delta_l z}{\rho} = \lim_{\Delta x \to 0} \frac{\Delta_x z}{\Delta x} = \frac{\partial z}{\partial x} \tag{2.5.3}$$

同理，如果$j$表示$y$轴的方向，那么根据方向函数的定义，当$e=j$的时候，由于$\Delta_l z = \Delta_y z, \rho=\Delta y$，从而：

$$\frac{\partial z}{\partial l} = \lim_{\rho \to 0} \frac{\Delta_l z}{\rho} = \lim_{\Delta y \to 0} \frac{\Delta_y z}{\Delta y} = \frac{\partial z}{\partial y} \tag{2.5.3}$$

多元函数的方向导数可以类比二元函数进行定义。

**定理** 如果函数$z=f(x,y)$在点$P(x,y)$可微，且在点$P(x,y)$处$f(x,y)$沿着任何方向$l$的方向导数都存在，并且$e=(cos\alpha, cos \beta)为l$上的单位向量，那么有：

$$\frac{\partial z}{\partial l} = \frac{\partial z}{\partial x} cos \alpha + \frac{\partial z}{\partial y} cos \beta$$

**定理** 如果函数$u=f(x,yz)$在点$P(x,y,z)$可微，且在点$P(x,y,z)$处$f(x,y,z)$沿着任何方向$l$的方向导数都存在，并且$e=(cos\alpha, cos \beta, cos\gamma)为l$上的单位向量，那么有：

$$\frac{\partial u}{\partial l} = \frac{\partial u}{\partial x} cos \alpha + \frac{\partial u}{\partial y} cos \beta + \frac{\partial u}{\partial z} cos \gamma \tag{2.5.3}$$

多元函数同样可以类比二元和三元函数建立方向导数和偏导数之间的联系。

**偏导数、方向导数、梯度的关系来了！**

式子(2.5.3)可以进一步写成向量点乘的形式：

$$\frac{\partial u}{\partial l} = (\frac{\partial u}{\partial x}, \frac{\partial u}{\partial y}, \frac{\partial u}{\partial z}) \cdot (cos \alpha, cos \beta, cos \gamma) \tag{2.5.4}$$

记$g=(\frac{\partial u}{\partial x}, \frac{\partial u}{\partial y}, \frac{\partial u}{\partial z}), e=(cos \alpha, cos \beta, cos \gamma)$

则$\frac{\partial u}{\partial l} = g \cdot e = |g|cos(g,e)$,因此可以知道当向量$e$与向量$g$方向一致时候取得最大值，即函数$u(x,y,z)$沿着此时方向$g$的方向导数取得最大值，且这个最大值为：

$$|g| = \sqrt{(\frac{\partial u}{\partial x})^2 + \frac{\partial u}{\partial y})^2+ \frac{\partial u}{\partial z})^2} \tag{2.5.5}$$

因此对于此时的$g$我们给出如下定义：

**梯度的定义**

**向量$(\frac{\partial u}{\partial x}, \frac{\partial u}{\partial y}, \frac{\partial u}{\partial z})$称为函数$u(x,y,z)在点P(x,y,z)处的梯度，记为\mathbf{grad} u$**,也就是：

$$\mathbf{grad} u =(\frac{\partial u}{\partial x}, \frac{\partial u}{\partial y}, \frac{\partial u}{\partial z}) \tag{2.5.6} $$

**梯度的意义：**

根据上面的讨论，函数$u(x,y,z)$的梯度的意义在于：假设$P(x,y,z)$是空间中一点，那么从点$P$所引出的所有射线方向中，沿着方向$\mathbf{grad} u$的方向导数最大，并且这个最大的方向导数值为$|\mathbf{grad} u|$

$$max\frac{\partial u}{\partial l} = |\mathbf{grad} u| = \sqrt{(\frac{\partial u}{\partial x})^2 + (\frac{\partial u}{\partial y})^2+ (\frac{\partial u}{\partial z})^2} \tag{2.5.7}$$

**举个例子**

假设$u(x,y,z)=x^2y + xy^2z$，求$\mathbf{grad} u(2,1,0)$.

$$\frac{\partial u}{\partial x} = 2xy + y^2z, \frac{\partial u}{\partial y} = x^2 + 2xyz, \frac{\partial u}{\partial z} = xy^2$$

在点(2,1,0)带入可以得到$\mathbf{grad} u(2,1,0) = 4i + 4j + 2k, i,j,k为沿着x,y,z的单位向量$

## 2.7 向量的梯度和Jacobian矩阵
上一章节我们学习了三元函数中梯度的定义和例子。对于三元函数$f(x,y,z)$，我们同样可以从向量的角度进行理解：函数$f: \mathbb R^3 \to R$的输入是一个3维的向量（我们具体表示成了$x,y,z$），输出是一个标量。同样的对于$N$元函数可以理解为：$f: \mathbb R^N \to R$，输入是一个$N$维的向量，输出是一个标量。

对于函数$f: 输入x=[x_1, x_2,...,x_N] \in \mathbb R^N，输出y$,结合方向导数的定义和梯度的定义，我们可以得到如下等式：
$$\lim_{\rho \to 0} \frac{\Delta_l y}{\rho} =  \bigtriangledown_x y =  \lim_{\rho \to 0} \frac{f(x_1+\Delta x_1,..., x_N+ \Delta x_N) - f(x_1,..., x_N)}{\rho} \tag{2.7.1}$$
$$\bigtriangledown_x y  = [\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2},..., \frac{\partial y}{\partial x_N}] \tag{2.7.2}$$
$$\rho = \sqrt{(\frac{\partial y}{\partial x_1})^2 + (\frac{\partial y}{\partial x_2})^2+,...,+ (\frac{\partial y}{\partial x_N})^2} \tag{2.7.3}$$

现在我们用符号$\bigtriangledown_x y \in \mathbb R^N$表示标量$y$对向量$x$的梯度，也是一个向量。和标量的梯度类似，我们可以得到：
$$x \to x + \Delta x \Rightarrow y \to \approx y + \bigtriangledown_x y \cdot \Delta x $$

与标量章节有所不同，我们从考虑标量自变量变化到考虑向量自变量$x \in \mathbb R^N$和自变量的变化$\Delta x \in \mathbb R^N$，因此得到对应的梯度/方向导数是向量$\bigtriangledown_x y \in \mathbb R^N$，最后我们将$\bigtriangledown_x y \in \mathbb R^N 点乘法 \Delta x $来反应因变量$y$的变化。$y$的变化受到$x$中每一维度变化的影响。

接下来继续考虑**输入输出都是向量**的情况：

现在，假设我们的函数$f: \mathbb R^N \to \mathbb R^M$的输入是一个$N$维的向量，输出是一个$M$维的向量。那么，函数$y=f(x) \in \mathbb R^M$对于输入$x \in \mathbb R^N$每一维标量的偏导数可以写成：

$$\frac{\partial y}{\partial x} = \begin{bmatrix}
 \frac{\partial y_1}{\partial x_1}& ... &  \frac{\partial y_1}{\partial x_N}\\ 
 .& ... &. \\ 
  \frac{\partial y_M}{\partial x_1}& ... &  \frac{\partial y_M}{\partial x_N}
\end{bmatrix} \tag{2.7.4}$$

式子(2.7.4)右边$M \times N$的偏导数矩阵又叫做Jacobian，我们也可以将上式理解为$M个标量y$对向量$x$求偏导。。Jacobian矩阵中连接了输入$x$和输出$y$中的每一个元素：Jacobian中的$(i,j)$个偏导$\frac{\partial y_i}{\partial x_j}$反应了$x_j$变化对$y_i$的影响。因此$y$向量与$x$向量的变化关系可以表达为：

$$x \to x + \Delta x \Rightarrow y \to \approx y + \frac{\partial y}{\partial x}  \Delta x$$

$\frac{\partial y}{\partial x}$是一个$M \times N$的矩阵，$\Delta x$是一个N维的向量，所以根据矩阵乘法得到的
$\frac{\partial y}{\partial x} \cdot \Delta x$是一个$M$维的向量，每一位对应着$M$维$y$的改变。

使用jacobian矩阵可以将标量中的链式法则拓展到向量计算中。假设函数$f: \mathbb R^N \to \mathbb R^M$和函数$g: \mathbb R^M \to \mathbb R^K$，输入$x \in \mathbb R^N$，经过函数$y=f(x)$输出中间变量$y \in \mathbb R^M$,再经过函数$z = g(y)$输出$z \in \mathbb R^K$:

$$x \overset{f} \to  y \overset{g} \to z $$

**链式法则**和标量时候一样：

$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$$

只不过此时的$\frac{\partial z}{\partial x} \in \mathbb R^{K \times N}$, $\frac{\partial z}{\partial y} \in \mathbb R^{K \times M}$, $\frac{\partial y}{\partial x} \in \mathbb R^{M \times N}$，进行的是矩阵乘法运算。




## 2.8 矩阵、张量的梯度

标量组成向量，向量进一步可以组成矩阵，矩阵再进一步组成了张量。所以现在我们进一步学习矩阵、张量的梯度（每一个位置求偏导）。

由于深度学习中的大部分运算的输入输出都是张量（tensor），比如输入的一张图像通常是$3 \times 宽度 \times 高度$，所以我们需要能够明白张量和张量之间的梯度是如何计算的。

类比输入输出都是向量的情况，假设函数$f: \mathbb R^{N_1 \times N_2 ... \times N_{D_x}} \to \mathbb R^{M_1 \times M_2 ... \times M_{D_y}}$的输入是一个$D_x$维的向量，形状是$N_1 \times N_2 ... \times N_{D_x}$，输出是$D_y$维度的向量$M_1 \times M_2 ... \times M_{D_y}$，对于这样的$y=f(x)$，**generalized Jacobian** $\frac{\partial y}{\partial x}$的形状是：
$$(M_1 \times M_2 ... \times M_{D_y}) \times (M_1 \times M_2 ... \times M_{D_y})$$

注意：我们将Jacobian矩阵的维度分成了两组：第一组是输出$y$的维度，第二组是输入$x$的形状。这样分组之后，我们可以将**generalized Jacobian**矩阵看成一个“假二维”的矩阵便于理解。这个“假二维”矩阵的每一行的形状与$x$相同，每一列的形状与$y$相同。

假设$i \in \mathbb Z ^{D_y}, j \in \mathbb Z ^{D_x}$ 是“假矩阵”的下标（下标不再是标量，而是一个向量来指明位置），我们有：

$$(\frac{\partial y}{\partial x})_{i,h} = \frac{\partial y_i}{\partial x_j} \tag{2.8.1}$$

式子(2.8.1)中的$y_i, x_j$都是标量，但$i,j是向量，表示具体的y和x在张量中的位置$， 因此$\frac{\partial y_i}{\partial x_j}$依旧是一个标量。有了上面的式子，我们进一步可以得到输入和输出的关系为：

$$x \to x + \Delta x \Rightarrow y \to \approx y + \frac{\partial y}{\partial x}  \Delta x \tag{2.8.2}$$

只不过此时$\Delta x$的维度是$N_1 \times N_2 ... \times N_{D_x}$, $$\frac{\partial y_i}{\partial x_j}的维度是: $(M_1 \times M_2 ... \times M_{D_y}) \times (M_1 \times M_2 ... \times M_{D_y})$, $\frac{\partial y}{\partial x}  \Delta x$的维度是：$M_1 \times M_2 ... \times M_{D_y}$。

式子(2.8.2)中的矩阵乘法和1.2章节中的矩阵乘法法则一样，暂且称之为**generalized matrix multiply**：

$$(\frac{\partial y}{\partial x}  \Delta x)_j = \sum_i (\frac{\partial y}{\partial x} )_{i,j} (\Delta x)_i = (\frac{\partial y}{\partial x} )_{j, :} \cdot \Delta x$$

和普通矩阵乘法唯一的不同就是下标$i,j$不再是标量，而是下标向量。$(\frac{\partial y}{\partial x} )_{j, :}$可以看作矩阵$\frac{\partial y}{\partial x}$的第$j$行，该行形状与$x$相同，因此直接和$\Delta x$进行elementwise乘法。

于是对于张量，我们同样有**链式法则**：假设$y=f(x), z=g(y)$, $x,y$形状如上，$z$的形状是$K_1,...K_{D_z}$,我们可以将张量链式法则写成：

$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$$

区别在于$\frac{\partial z}{\partial x}$的维度是$(K_1 \times K_2 ... \times K_{D_z}) \times (N_1 \times N_2 ... \times N_{D_x})$,$\frac{\partial z}{\partial y}$的维度是$(K_1 \times K_2 ... \times K_{D_z}) \times (M_1 \times M_2 ... \times M_{D_y})$,$\frac{\partial y}{\partial x}$的维度是$(M_1 \times M_2 ... \times M_{D_y}) \times (N_1 \times N_2 ... \times N_{D_x})$。


从**generalized matrix multiply**：角度，我们可以得到：
$$(\frac{\partial y}{\partial x})_{i,j} = \sum_k (\frac{\partial y}{\partial x} )_{i,k} (\frac{\partial y}{\partial x})_i = (\frac{\partial z}{\partial y} )_{i, :} \cdot (\frac{\partial y}{\partial x} )_{:, j}$$

同样，此时的$i,j,j$不再是标量，而是向量。

## 2.9 神经网络中几个实用的梯度计算

2.8章节我们知道了张量求梯度的方法以及张量中的链式法则，这一章节我们结合Jacobian矩阵继续学习神经网络中几个非常使用的梯度计算。

### 2.9.1 $z = Wx$矩阵$W \in \mathbb R^{n \times m}$乘以列向量$x \in \mathbb R^m$求$\frac{\partial z}{\partial x}$

可以看作函数将输入$x \in \mathbb R^m$ 经过$W \in \mathbb R^{n \times m}$变换得到输出$z \in \mathbb R^n$,那么Jacobian矩阵$\frac{\partial z}{\partial x} \in \mathbb R^{n \times m}$

$$z_i = \sum^{m}_k W_{ik}x_k$$

那么
$$(\frac{\partial z}{\partial x})_{i,j} = \frac{\partial z_i}{\partial x_j} = \frac{\partial {\sum^{m}_k W_{ik}x_k}}{\partial x} = \sum^{m}_k W_{ik} \frac{\partial x_k}{\partial x_j} = W_{ij}$$

由于$\frac{\partial x_k}{\partial x_j} = 1$ if $k = j$ else 0， 所以有$\frac{\partial z}{\partial x} = W$

### 2.9.2 $z=xW, \frac{\partial z}{\partial x} = W^T$

### 2.9.3 $z=x$向量等于自身,求$\frac{\partial z}{\partial x}$

因为$z_i = x_i$ 所以
$$(\frac{\partial z}{\partial x})_{i,j} = \frac{\partial z_i}{\partial x_j} = \frac{\partial x_i}{\partial x_j} = \{^{1, i=j}_{0, otherwise}$$

所以$\frac{\partial z}{\partial x} = I$,将其放在链式法则中进行矩阵乘法时候不会改变其他矩阵。

### 2.9.4 $z = f(x)$ 对向量$x$中每个元素进行变换, 求$\frac{\partial z}{\partial x}$
由于$z_i = f(x_i)$所以
$$(\frac{\partial z}{\partial x})_{i,j} = \frac{\partial z_i}{\partial x_j} = \frac{\partial f(x_i)}{\partial x_j} = \{^{f'(x_i), i=j}_{0, otherwise}$$

所以$\frac{\partial z}{\partial x}$是一个diagonal matrix 且$\frac{\partial z}{\partial x} = diag(f'(x))$

矩阵乘以一个diagonal矩阵也就是每个元素进行幅度变换，因此链式法则中的矩阵乘以$diag(f'(x))$相当于和$f'(x)$做elementwise 乘法。

### 2.9.5 $z = Wx, \delta = \frac{\partial J}{\partial z}$，求$\frac{\partial J}{\partial W}=\frac{\partial J}{\partial z} \frac{\partial z}{\partial W}=\delta \frac{\partial z}{\partial W}$

我们开始引入更复杂的情况，因为神经网络中往往包含多次链式法则的引用，这里我们假设已经知道$\delta = \frac{\partial J}{\partial z}$，直接求$\frac{\partial J}{\partial W}$。

假设神经网络的损失函数$J$是标量，我们想计算的是损失函数对参数$W \in \mathbb R^{n \times m}$的梯度。我们可以想象神经网络这个函数输入是一个$n \times m$形状的参数，输出是一个标量，结合上一章节Jacobian知识我们可以知道$\frac{\partial J}{\partial W} \in \mathbb R^{(1) \times (n \times m)}$形状和$W$一样，所以在神经网络训练的时候可以将参数减轻去参数的梯度乘以学习率。

根据链式法则，我们需要求出$\frac{\partial z}{\partial W} \in \mathbb R^{(n) \times (n \times m)}$。这个三维的张量不方便表示且十分复杂，因此我们先只看对$W_{i,j}$求导$\frac{\partial z}{\partial W_{i,j}} \in \mathbb R^{(n) \times (1)}$。

$$z_k = \sum^m_{l=1}  W_{kl}x_l$$
$$\frac{\partial z}{\partial W_{ij}} = \sum^m_{l=1}x_l \frac{\partial W_{kl}}{\partial W_{ij}}$$

$$\frac{\partial W_{kl}}{\partial W_{ij}} = \{^{1, i=k, j=l}_0$$

所以只有$i=k, j=l$时候非零
$$\frac{\partial J}{\partial W_{ij}} = \begin{bmatrix}
0\\ 
.\\ 
x_j\\ 
.\\ 
0
\end{bmatrix} \leftarrow  ith element, j=l$$

所以
$$\frac{\partial J}{\partial W_{ij}} = \frac{\partial J}{\partial z}\frac{\partial z}{\partial W_{ij}} = \delta \frac{\partial z}{\partial W_{ij}} = \delta_i x_j$$

所以得到
$$\frac{\partial J}{\partial W} = \delta^T x^T $$

### 2.9.6 $z=xW, \delta = \frac{\partial J}{\partial z}, \frac{\partial J}{\partial W} =x^T\delta$

### 2.9.7 $\hat{y} =  softmax(\Theta)， J=Cross-entropy(y,\hat{y}), \frac{\partial J}{\partial \Theta } = \hat{y} - y$

假设神经网络到达softmax之前的输出为$\Theta = [\theta_1, \theta_2, ..., \theta_n]$,$n$为分类数量，那么
$$\hat{y} = softmax(\Theta), \theta_i = \frac{e^{\theta_i}}{\sum^n_k e^{\theta_k}}$$
$$J = \sum^n_{i=1}y_ilog(\hat{y_i})$$
$$\frac{\partial J}{\hat{y_i}} = \frac{y_i}{\hat{y_i}}$$
$$\frac{\partial \hat{y_i}}{\theta_i} = \{^{\hat{y_i}(1-\hat{y_k}), i=k}_{-\hat{y_i}\hat{y_k}, i \neq k}$$

$$\frac{\partial J}{\theta_i} = \frac{\partial J}{\hat{y}} \frac{\partial \hat{y}}{\theta_i}, 形状为1 \times n, n \times 1$$

$$\frac{\partial J}{\theta_i} = \sum^n_{k=1}{\frac{\partial J}{\hat{y_k}} \frac{\partial \hat{y_k}}{\theta_i}} (k=i, k\neq i)$$

$$\frac{\partial J}{\theta_i} = \frac{\partial J}{\hat{y_i}} \frac{\partial \hat{y_i}}{\theta_i} + \sum^n_{k \neq  i}{\frac{\partial J}{\hat{y_k}} \frac{\partial \hat{y_k}}{\theta_i}} (k=i, k\neq i)$$

$$\frac{\partial J}{\theta_i} = y_i(1- \hat{y_i}) + \sum_{k neq i} {y_k \hat{y_i}}$$

$$\frac{\partial J}{\theta_i} = -y_i(1- \hat{y_i}) + \sum_{k neq i} {y_k \hat{y_i}}$$

$$\frac{\partial J}{\theta_i} = -y_i + \hat{y_i} \sum {y_k} $$

$$\frac{\partial J}{\theta_i} = \hat{y_i} -y_i  $$

所以
$$\frac{\partial J}{\theta} = \hat{y} -y \in \mathbb R^{n}  $$



## 3.1 神经网络中的反向传播

神经网络可以理解为输入张量$x$和权重张量$W$的一个巨大函数$y=f(x, W)$。函数的输出在和标注（正确答案）比较得到一个标量的损失函数$J$。因此我们反向传播的目的是求$\frac{\partial J}{\partial x }, \frac{\partial J}{\partial W }$，根据链式法则：
$$\frac{\partial J}{\partial x } = \frac{\partial J}{\partial y }\frac{\partial y}{\partial x }, \frac{\partial J}{\partial W } = \frac{\partial J}{\partial y } \frac{\partial y}{\partial W }$$

在求得参数$W$的梯度之后，将参数根据学习率和梯度进行迭代更新 $W = W - \alpha \frac{\partial y}{\partial W }$寻找最佳参数。

## 3.2 单层神经网络的梯度计算例子
在本章节，我们来对一个单层神经网路求梯度验证我们对导数、矩阵运算的学习效果。为了能够对神经网络复杂的计算求导，我们将神经网络的步步拆开成下面的表达式来看。单层神经网络可以表示为：
$$x = input \in \mathbb R^{D_x \times 1}$$
$$z = Wx + b_1 \in \mathbb R^{D_h \times 1}$$
$$h = ReLU(z) \in \mathbb R^{D_x \times 1}$$
$$\Theta = Uh + b_2 \in \mathbb R^{N_c \times 1}$$
$$\hat{y} = softmax(\Theta ) \in \mathbb R^{N_c \times 1}$$
$$J = cross-entropy(y, \hat{y} \in \mathbb R^{1}$$
其中
$$x \in \mathbb R^{D_x \times 1}, b_1 \in \mathbb R^{D_h \times 1}, W \in \mathbb R^{D_h \times D_x}, b_2 \in \mathbb R^{N_c \times 1}, U \in \mathbb R^{N_c \times D_h}$$

$D_x, D_h$分别是输入和hidden layer的大小，$N_c$是分类的类别数量。

我们想计算的是损失函数对参数$U,W, b_1, b_2$的梯度，我们可以计算的梯度有：
$$\frac{\partial J}{\partial U}, \frac{\partial J}{\partial b_2}, \frac{\partial J}{\partial W}, \frac{\partial J}{\partial b_1}, \frac{\partial J}{\partial x}$$

另外我们需要知道$ReLu(x) = max(x, 0)$,他的导数：
$$ReLU'(x) = \{^{1, x>0}_{0} = sgn(ReLu(x))$$

sgn是符号函数（输入大于0为输出1，输入小于等于0输出0）。

假设我们要求$U, b_2$的梯度，我们可以写出链式法则：
$$\frac{\partial J}{\partial U} = \frac{\partial J}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \Theta} \frac{\partial \Theta}{\partial U}$$
$$\frac{\partial J}{\partial b_2} = \frac{\partial J}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \Theta} \frac{\partial \Theta}{\partial b_2}$$

从上面的两个式子可以看出$\frac{\partial J}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \Theta}$被用了两遍，所以我们再定义一个中间变量：
$$\delta_1 = \frac{\partial J}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \Theta} =  \frac{\partial J}{\partial \Theta}$$

根据2.9.7的推到可以得到
$$\delta_1 = \frac{\partial J}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \Theta} =  \frac{\partial J}{\partial \Theta} = (\hat{y} - y)^T \mathbb R^{1 \times N_c}$$

进一步对$z$求梯度：

$$\frac{\partial J}{\partial z} = \frac{\partial J}{\partial \Theta}  \frac{\partial \Theta}{\partial h} \frac{\partial h}{\partial z}$$

$$\frac{\partial J}{\partial z} = \delta_1 \frac{\partial \Theta}{\partial h} \frac{\partial h}{\partial z}, 带入\delta_1$$

$$\frac{\partial J}{\partial z} = \delta_1 U \frac{\partial h}{\partial z}, 根据2.9.1$$

$$\frac{\partial J}{\partial z} = \delta_1 U \odot ReLu'(z), 根据2.9.4， \odot代表elementwise乘法$$

$$\frac{\partial J}{\partial z} = \delta_1 U \odot sgn(h)$$

最后进行一下形状检查
$$\frac{\partial J}{\partial z} (1 \times D_h) =  \delta_1(1 \times N_c)乘以 U (N_c \times D_h)\odot sgn(h) (D_h)$$

## 4.1 Pytorch自动求梯度

```
import torch

x = torch.arange(4.0)
print(x)  #tensor([0., 1., 2., 3.])
x.requires_grad_(True)  # 设定求梯度
print(x.grad)  # 默认值是None
y = 5 * torch.dot(x, x) #计算输出
print(y) #tensor(150., grad_fn=<MulBackward0>)

y.backward() #反向传播
print(x.grad) # tensor([ 0., 10., 20., 30., 40.])
print(x.grad == 10 * x) # tensor([True, True, True, True, True])
```

## 4.2 Tensorflow自动求梯度
```
import tensorflow as tf

x = tf.range(4, dtype=tf.float32)
print(x) #tf.Tensor([0. 1. 2. 3.], shape=(4,), dtype=float32)
x = tf.Variable(x)

with tf.GradientTape() as t:
    y = 5 * tf.tensordot(x, x, axes=1)
x_grad = t.gradient(y, x)
x_grad
print(y) #tf.Tensor(70.0, shape=(), dtype=float32)
print(x_grad == 10 * x) #tf.Tensor([ True  True  True  True], shape=(4,), dtype=bool)
```









