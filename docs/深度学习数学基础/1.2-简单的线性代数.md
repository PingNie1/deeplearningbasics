

## 1.2-简单的线性代数


1.1章节中我们谈到向量可以延伸到矩阵，然后延伸到张量，因此为了能够更好得对深度学习中的向量/矩阵/张量运算、梯度求解有一个更清晰的理解，我们需要学习一下基本的线性代数知识。由于张量运算过于复杂，一般我们将张量运算交给计算机进行。我们在进行线性代数讲解的时候也尽可能使用特殊而又简单的张量（也就是矩阵和向量）。

### 什么是线性代数

一般来说，线性代数在是本科一年级的一门必修课。但是本文将不会涉及完整的线性代数体系知识，仅仅将线性代数常用于深度学习的部分知识进行整理。我们将线性代数看作深度学习中的数学计算工具即可：深度学习使用线性代数对张量进行加减乘除等各种运算。

### 线性代数能带来什么好处？

由于深度学习中需要进行大量的张量运算，而每个张量又包含了多个标量，因此可以想象，如果直接进行标量计算需要大量的$for$循环。但是线性代数的知识，能将复杂而又慢的循环遍历标量计算变得更加优雅、快速、简单。

### 例子
```
# 将两个向量中的每个元素相乘求和
x = [1,2,3]
y = [2,3,4]
sum = 0.0
for i in range(len(x)):
    sum = sum + x[i]*y[i]
# 借助深度学习工具torch，使用点乘直接运算
import torch
x = torch.tensor([1,2,3])
y = torch.tensor([2,3,4])
sum = torch.dot(x, y))
```

### 深度学习中的应用

深度学习中的神经网络包含了大量的参数（也叫做parameters/weights）和参数计算。参数往往以张量的形式储存，线性代数的知识（比如点乘，矩阵运算）能让参数间的计算更加快速（如果是使用GPU进行计算，这种提速会更加明显）。

### 标量运算

最简单的形式：标量和标量进行加减乘除运算。

*例子：标量+标量*

```
import torch
a = torch.tensor(2)
b = torch.tensor(3)
c = a + b
print(c)
# tensor(5)
```

### 向量和标量

将向量中的每一个标量都与另一个标量都进行计算（[broadcasting/广播](https://pytorch.org/docs/stable/notes/broadcasting.html)的一种）。

*例子*

```
a = torch.tensor(2)
b = torch.tensor([1,2,3])
c = a + b
print(c)
# tensor([3, 4, 5])
```

### 向量和向量按位置（elementwise）运算

向量间同一个位置的元素/标量进行运算。

*例子*
```
a = torch.tensor([2,3,4])
b = torch.tensor([1,2,3])
c = a + b
print(c)
# tensor([3, 5, 7])
# Hadamard product
a = torch.tensor([2,3,4])
b = torch.tensor([1,2,3])
c = a * b
print(c)
# tensor([ 2,  6, 12])
```

### 点乘

向量之间的点乘是深度学习中最普遍也是最常用的一种计算:先按位置做乘法，然后相加求和。

*例子*
```
a = torch.tensor([2,3,4])
b = torch.tensor([1,2,3])
c = torch.dot(a, b)
print(c)
# tensor(20)
```

### 矩阵和标量

将矩阵中的每一个标量都与另一个标量都进行计算（[broadcasting/广播](https://pytorch.org/docs/stable/notes/broadcasting.html)的一种）。

*例子*

```
a = torch.tensor(2)
b = torch.tensor([[1,2,3],
                  [1,2,3]])
c = a + b
print(c)
# tensor([[3, 4, 5],
#        [3, 4, 5]])
```

### 矩阵和向量

将向量广播成与矩阵相同的维度，然后和矩阵的每一个元素都进行计算（[broadcasting/广播](https://pytorch.org/docs/stable/notes/broadcasting.html)的一种）。

*例子*

```
a = torch.tensor(2)
b = torch.tensor([[1,2,3],
                  [1,2,3]])
c = a + b
print(c)
# tensor([[3, 4, 5],
#        [3, 4, 5]])
```

### 矩阵和矩阵按位置（elementwise）运算

向量间同一个位置的元素/标量进行运算。

*例子*
```
a = torch.tensor([[2,3,4],
                  [2,3,4]])
b = torch.tensor([[1,2,3]])
# 现将b变成[[1,2,3],
#          [1,2,3]]再计算
c = a + b
print(c)
# tensor([[3, 5, 7],
#        [3, 5, 7]])
# Hadamard product
```

### 矩阵乘法

深度学习中最常用、最重要的计算！
假设矩阵$a \in \mathbb R ^{n \times m}, b \in \mathbb R^{m \times k}$，那么$c = a \times b \in \mathbb R^{n \times k}$。对于$c$矩阵中的每一个元素计算方式为：$a$的第$i$行向量与$b$的第$j$列向量进行点乘。

$$c_{ij} = \sum_k {a_{ik}b_{kj}}$$

因此两个矩阵要进行矩阵乘法必须满足：前一个矩阵的列数=后一个矩阵的行数。

*例子*
```
a = torch.tensor([[2,3,4],
                  [2,3,4]])
print(a.shape)
# torch.Size([2, 3])
b = torch.tensor([[1],
                  [2],
                  [3]])
print(b.shape)
# torch.Size([3, 1])
c = torch.matmul(a, b)
print(c)
#tensor([[20],
#        [20]])
print(c.shape)
# torch.Size([2, 1]) 
```
### 转置

前面提到了矩阵乘法，因此为了能够让矩阵乘法正常计算，我们常常需要对矩阵进行转置来得到正确的矩阵形状。比如一个$\mathbb R^{2 \times 3}$的矩阵无法和$\mathbb R^{2 \times 3}$直接做乘法，但可以将后一个转置得到$\mathbb R^{3 \times 2}$后便可以进行矩阵乘法了。具体的如何进行转置呢？

假设矩阵为
$$M = \begin{bmatrix}
 [a& b &c], \\ 
 [d& e & f]
\end{bmatrix}$$

那么它的转置为
$$M = \begin{bmatrix}
 [a& d], \\ 
 [b& e], \\
 [c&f ]
\end{bmatrix}$$

- 先将矩阵$M$逆时针旋转$90^0$
- 然后每一列顺序反过来即可，比如第一列：$[b,a,c] \to [a,b,c]$

```
import torch
a = torch.tensor([[2,3,4],
                  [2,3,4]])
print(a.shape)
b = torch.tensor([[2,3,4],
                  [2,3,4]])
print(b.shape)
c = b.T
print(c.shape)
d = torch.matmul(a, c)
print(d)
# torch.Size([2, 3])
# torch.Size([2, 3])
# torch.Size([3, 2])
# tensor([[29, 29],
#        [29, 29]])
```

参考文章：[linear-algebra-cheat-sheet-for-deep-learning](https://towardsdatascience.com/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)
