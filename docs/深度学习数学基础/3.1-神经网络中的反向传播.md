[返回首页](../README.md)

## 3.1神经网络中的反向传播

神经网络可以理解为输入张量$x$和权重张量$W$的一个巨大函数$y=f(x, W)$。函数的输出在和标注（正确答案）比较得到一个标量的损失函数$J$。因此我们反向传播的目的是求$\frac{\partial J}{\partial x }, \frac{\partial J}{\partial W }$，根据链式法则：
$$\frac{\partial J}{\partial x } = \frac{\partial J}{\partial y }\frac{\partial y}{\partial x }, \frac{\partial J}{\partial W } = \frac{\partial J}{\partial y } \frac{\partial y}{\partial W }$$

在求得参数$W$的梯度之后，将参数根据学习率和梯度进行迭代更新 $W = W - \alpha \frac{\partial y}{\partial W }$寻找最佳参数。