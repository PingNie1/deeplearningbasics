[返回首页](../README.md)

## 1.1-标量、向量、矩阵、张量
**标量** 简单来说：一个标量就是一个单独的数。
日常生活的一个例子：我们去超市买了1个冰淇凌，花费了2元，排队结账用了5分钟，其中1、2和5都是标量。

对于标量，我们一般不讨论其维度或者形状(shape)的大小（或者说是0维）。

**向量**
将多个标量放在一起组成了向量。比如$x=[1,2]$就是一个向量。一个二维平面的点位置有2个特征：横坐标和纵坐标，因此可以用$[x坐标，y坐标]$这样一个二维向量来表示二维平面上的一个点；同理，可以使用[x坐标，y坐标，z坐标]三维向量来表示空间中的一个点所在的位置。日常生活中，我们也可以使用$[身高h,性别s,体重w,肤色f]$这样一个4维向量来表示一个人的特点。更高维的向量可以表达一个物体更多维度的特征。

向量的维度通常表达的是向量中标量的个数。$[1,2，3]$的维度为3维。

**矩阵** 将多个向量排列到一起可以组成矩阵。例如：
$$\begin{bmatrix}
[1 & 2], \\
[1 & 3] 
\end{bmatrix} \tag{1-1-1}$$

如矩阵(1-1-1)所示，矩阵的每一行就是一个向量。

矩阵的**形状**：每一列标量的个数 x 每一行标量的个数，矩阵(1-1-1)形状为2x2，维度为2。

**张量** 
将多个矩阵组合到一起可以形成张量。比如：
$$\begin{bmatrix} 
\begin{bmatrix}
[1 & 2] \\
[1 & 2] 
\end{bmatrix}, \\
\begin{bmatrix}
[2 & 3] \\
[2 & 3] 
\end{bmatrix}, \\
\begin{bmatrix}
[3 & 3] \\
[3 & 4] 
\end{bmatrix} \\
\end{bmatrix} \tag{1-1-2}$$
因此标量、向量、矩阵都可以看作是维度更少的张量。

张量的形状：假设张量A形状为$d_n\times d_{n-1}... \times d_1$，那么表达的含义是：$n$维张量A中包含了$d_{n}$个形状为$d_{n-1} ... \times d_1$的$n-1$维张量B，并以此类推到1维张量。所以张量(1-1-2)的形状是3x2x2维度是3，张量(1-1-1)的形状是是2x2维度是2。

注意：由于大家的对矩阵、张量的形状和维度的概念容易混淆，本文在描述张量形状的时候都会使用$\times$符号来区分不同的维度。

**Pytorch实践**
```
import torch
scalar = torch.tensor(5)
vector = torch.tensor([5,5,5])
matrix = torch.tensor([[5,2],[5,3]])
tensor = torch.tensor([[[5,2],[5,3]], [[1,2],[2,3]]])
print("scalar: {}, shape: {}".format(scalar, scalar.shape))
print("vector: {}, shape: {}".format(vector, vector.shape))
print("matrix: {}, shape: {}".format(matrix, matrix.shape))
print("tensor: {}, shape: {}".format(tensor, tensor.shape))
"""
scalar: 5, shape: torch.Size([])
vector: tensor([5, 5, 5]), shape: torch.Size([3])
matrix: tensor([[5, 2],
        [5, 3]]), shape: torch.Size([2, 2])
tensor: tensor([[[5, 2],
         [5, 3]],

        [[1, 2],
         [2, 3]]]), shape: torch.Size([2, 2, 2])
"""
```